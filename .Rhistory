ggtsdisplay(Bz, lag.max = 48) #Lag en el 12 -> Serie temporal mensual
# ? - Parte II: Diferenciación de Tendencia  - D
B12z <- diff(y, lag = 12, differences = 1)
ggtsdisplay(B12z, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte III: Diferenciación Regular y de Tendencia (Esto es que haces primero Parte I, y después le aplicas Parte II) - d y D
B12Bz <- diff(Bz, lag = 12, differences = 1)
ggtsdisplay(B12Bz, lag.max = 4 * 12)
# Nada nada, tiramos con la diferenciación regular, la estacionaria no hace falta.
#* 7. Apply Box-Cox? Check if R^2 > 0.03 - Objetivo: Estabilizar la varianza
Lambda <- BoxCox.lambda.plot(y, window.width = 12)
# Lambda <- BoxCox.lambda(y)
# No es necesario hacer Box-Cox debido a que es muy pequeño su R^2 para su ventana correspondinete.
# Ahora debemos de separar entre TR y TEST
y.TR <- subset(y, end=length(y)*0.8)
y.TV <- subset(y, start = length(y)*0.8 + 1)
x.TR <- subset(x, end = length(x)*0.8)
x.TV <- subset(x, start = length(x)*0.8 + 1)
# Re-lee la pregunta y ahora generamos nuestro primer modelo LTF
# Anda mira, y si lo hubiesemos leído tambien nos hubiesemos dado cuenta que las
# series temporales eran mensuales con ventana 12 HAHAHAHHAHAHA
#Problem 1: (5 points)
#The dataset Prob1.dat contains two monthly time series: x and y.
#Questions:
#  1. Identify a transfer function model for time series y using x as an input.
#  2. Diagnose your proposed model <- Esto que significa especificamente?
#   TODO: If the regression errors are stationary, identify the transfer function α(L) by selecting the appropriate b, r, s values.
#   * To identify b, analyze the number of initial non-significant coefficients (α_o,α_1,…,α_(b-1))
#   * The value of r determines the pattern of decay of the coefficients α_i:
#     ? If there is no pattern of decays, but a set of coefficients which then sharply drop down, r = 0.
#     ? If the pattern of decay is exponential, r = 1.
#     ? If the pattern of decay is a damped exponential or sine wave, we take r=2.
#   * The value of s determines the number of non-null α_i coefficients before the decay.
ggCcf(y,x)
gc()
# TODO: Tienes que incorporar los forecasts para ARIMA, SARIMA, LTF, MLP y GARCH
# * 1. Cargar librerias necesarias - aqui hay una lista de todas las usadas en clase
library(tseries)
library(fpp2)
library(tidyverse)
library(readxl)
library(MLTools)
library(ggplot2)
library(lmtest)
library(zoo)
library(forecast)
library(TSA)
library(Hmisc) # for computing lagged variables + NARX Model
library(NeuralSens)
library(caret)
library(kernlab)
library(nnet)
library(NeuralNetTools)
library(rugarch)   # Contains functions to fit ARMA+GARCH models
library(xts)       # eXtensible Time Series, used by rugarch
library(lubridate) # Easy manipulation of dates
library(quantmod)  # Retrieve financial data % Optional
fdata <- read.table("Prob1.dat", header = TRUE) # Si el archivo .dat chueco.
head(fdata)
fdata_ts <- ts(fdata)
y <- fdata_ts[,2]
x <- fdata_ts[,1]
ggtsdisplay(y)
# Regular differencing for sure.
ggtsdisplay(x)
# -> Pero aqui primero debemos averiguar que ventana tiene antes de hacer box cox
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 48) #Lag en el 12 -> Serie temporal mensual
# ? - Parte II: Diferenciación de Tendencia  - D
B12z <- diff(y, lag = 12, differences = 1)
ggtsdisplay(B12z, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte III: Diferenciación Regular y de Tendencia (Esto es que haces primero Parte I, y después le aplicas Parte II) - d y D
B12Bz <- diff(Bz, lag = 12, differences = 1)
ggtsdisplay(B12Bz, lag.max = 4 * 12)
#* 7. Apply Box-Cox? Check if R^2 > 0.03 - Objetivo: Estabilizar la varianza
Lambda <- BoxCox.lambda.plot(y, window.width = 12)
# Ahora debemos de separar entre TR y TEST
y.TR <- subset(y, end=length(y)*0.8)
y.TV <- subset(y, start = length(y)*0.8 + 1)
x.TR <- subset(x, end = length(x)*0.8)
x.TV <- subset(x, start = length(x)*0.8 + 1)
ggCcf(y,x)
# Observamos una componente causal muy fuerte en la derecha
autoplot(fdata_ts, facets = TRUE)
ggtsdisplay(y) # ? - Para observar los ACF y PACF
ggtsdisplay(x)
# Debemos de escalar x e y para que quepa en el rango apropiado. Particularmente y entre [50 , -50]
y.TR <- y.TR/10
y.TV <- y.TV/10
ggtsdisplay(y.TR)
# * 3. Initial TF model
TF.fit <- arima(y.TR,
order = c(2, 1, 0),
seasonal = list(order=c(0,0,2),period=12), #! - Change the period
xtransf = x.TR,
transfer = list(c(0,9)), # * List with (r,s) orders - If there are multiple x inputs, add multiple (r,s)
include.mean = TRUE,
method = "ML") #Maximum likelihood as a fitting method.
barplot(coef(TF.fit)[-(1:4)], las=2, col='darkblue') # The [-(1:2)] removes the ar1 and the incercept and just leaves us with the T1-MA
summary(TF.fit) # summary of training errors and estimated coefficients
coeftest(TF.fit)
# * 4.Check regression error to see the need of differentiation
TF.RegressionError.plot(y.TR,x.TR,TF.fit,lag.max = 50)
# * 5. Check numerator coefficients of explanatory variable
TF.Identification.plot(x,TF.fit)
TF.fit <- arima(y.TR,
order = c(2, 1, 0),
seasonal = list(order=c(0,0,1),period=12), #! - Change the period
xtransf = x.TR,
transfer = list(c(1,1)), # * List with (r,s) orders - If there are multiple x inputs, add multiple (r,s)
include.mean = TRUE,
method = "ML") #Maximum likelihood as a fitting method.
barplot(coef(TF.fit), las=2, col='darkblue') # The [-(1:2)] removes the ar1 and the incercept and just leaves us with the T1-MA
summary(TF.fit) # summary of training errors and estimated coefficients
coeftest(TF.fit)
# * 7. Check residuals
CheckResiduals.ICAI(TF.fit, lag=50) #! Nice, white noise achieved :-)
ggtsdisplay(TF.fit$residuals)
#* 8. Check Cross correlation residuals - expl. variables
res <- residuals(TF.fit)
res[is.na(res)] <- 0
ccf(y = res, x = x.TR)
# Diagnosis del modelo generado:
autoplot(y.TR, series='Data') +
autolayer(fitted(TF.fit), series = "FIT")
#Obtain forecast for horizon = 7 using the trained parameters of the model
val.forecast_h12 <- TF.forecast(y.old = y.TR, #past values of the series
x.old = x.TR, #Past values of the explanatory variables
x.new = x.TV, #New values of the explanatory variables
model = TF.fit, #fitted transfer function model
h=12) #Forecast horizon
tail(y.TR*10)
val.forecast_h12*10
# Sigue una continuacion logica para el forecast:
autoplot(cbind(y.TV, x.TV), facets=TRUE)
y.TV.est <- rep(NA, length(y.TV))
for (i in seq(length(y.TR)+1, length(y), 1)){
y.TV.est[i] <- TF.forecast(y.old = subset(y, end=i-1),
x.old = subset(x, end = i-1),
x.new = subset(x, start = i, end = i),
model = TF.fit,
h = 1)
}
y.TV.est <- na.omit(y.TV.est)
head(y.TV.est)
head(y.TV)
accuracy(y.TV.est, y.TV*10)
# RMSE - 10.11478
plot(y.TV*10)
plot(y.TV.est)
#Plot series and forecast (! Important)
autoplot(y.TV*10)+
forecast::autolayer(ts(y.TV.est))
# Bueno, hora de pasar al siguiente ejercicio que se me hace tarde:
#Problem 2: (5 points)
#The dataset Prob2.dat contains two non-seasonal time series: the input x and the output y.
#Questions:
#  1. Identify a nonlinear model to predict one step ahead values of the output variable y,
#     using 80% of the data.
#  2. Diagnose your proposed model.
#  3. Validate your model using the remaining 20% of the data.
fdata_mlp <- read.table("Prob2.dat", header = TRUE) # Si el archivo .dat chueco.
head(fdata_mlp)
fdata_ts_mlp <- ts(fdata_mlp)
y.mlp <- fdata_ts_mlp[,2]
x.mlp <- fdata_ts_mlp[,1]
y.mlp.TR <- subset(y.mlp, end = length(y.mlp)*0.8)
y.mlp.TV <- subset(y.mlp, start = length(y.mlp)*0.8 + 1 )
x.mlp.TR <- subset(x.mlp, end = length(x.mlp)*0.8)
x.mlp.TV <- subset(x.mlp, start = length(x.mlp)*0.8 + 1)
"contains two non-seasonal time series"
# Genial, vamos a ver ahora. Dudas dudas.
ggtsdisplay(y)
# Vale, aqui va a hacer falta diferenciacion como la copa de un pino. <- Que expresion tan curiosa.
ggtsdisplay(x)
fdata.REG <- fdata_ts[]
#"contains two non-seasonal time series"
# Genial, vamos a ver ahora. Dudas dudas.
ggtsdisplay(y.mlp.TR)
# Vale, aqui va a hacer falta diferenciacion como la copa de un pino. <- Que expresion tan curiosa.
ggtsdisplay(x.mlp.TR)
fdata_mlp.REG <- fdata_ts_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg <- fdata_ts_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
fdata_mlp.Reg <- fdata_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x.mlp.TR,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x.mlp.TR,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x.mlp.TR,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y.mlp.TR,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y.mlp.TR,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y.mlp.TR,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
fdata.Reg.tr <- fdata.Reg
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x.mlp.TR,1)
# Bueno, hora de pasar al siguiente ejercicio que se me hace tarde:
#Problem 2: (5 points)
#The dataset Prob2.dat contains two non-seasonal time series: the input x and the output y.
#Questions:
#  1. Identify a nonlinear model to predict one step ahead values of the output variable y,
#     using 80% of the data.
#  2. Diagnose your proposed model.
#  3. Validate your model using the remaining 20% of the data.
fdata_mlp <- read.table("Prob2.dat", header = TRUE) # Si el archivo .dat chueco.
head(fdata_mlp)
fdata_ts_mlp <- ts(fdata_mlp)
y.mlp <- fdata_ts_mlp[,2]
x.mlp <- fdata_ts_mlp[,1]
y.mlp.TR <- subset(y.mlp, end = length(y.mlp)*0.8)
y.mlp.TV <- subset(y.mlp, start = length(y.mlp)*0.8 + 1 )
x.mlp.TR <- subset(x.mlp, end = length(x.mlp)*0.8)
x.mlp.TV <- subset(x.mlp, start = length(x.mlp)*0.8 + 1)
#"contains two non-seasonal time series"
# Genial, vamos a ver ahora. Dudas dudas.
ggtsdisplay(y.mlp.TR)
# Vale, aqui va a hacer falta diferenciacion como la copa de un pino. <- Que expresion tan curiosa.
ggtsdisplay(x.mlp.TR)
# Esto que no sea la time series
fdata_mlp.Reg <- fdata_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y.mlp, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y.mlp, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x.mlp, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x.mlp.TR,1)
# Let's go! (Vale, entonces, e)
fdata_mlp.Reg$x_lag1 <- Lag(x.mlp.TR,1)
fdata_mlp.Reg$x_lag2 <- Lag(x.mlp.TR,2)
fdata_mlp.Reg$x_lag3 <- Lag(x.mlp.TR,3)
# Let's go! (Vale, entonces esto se tiene que hacer con la columna entera, no la time series de entrenamiento)
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
fdata.Reg.tr <- fdata.Reg
#Remove missing values
fdata.Reg.tr <- na.omit(fdata.Reg.tr)
fdata_mlp.Reg.tr <- fdata_mlp.Reg
#Remove missing values
fdata_mlp.Reg.tr <- na.omit(fdata_mlp.Reg.tr)
fdata_mlp.Reg.tr
# * 3. Set up the Cross-Validation with 10 folds
ctrl_tune <- trainControl(method = "cv",
number = 10,
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples.
savePredictions = TRUE)              #save predictions
# * 4. Neural Network setup [TRAINING]
#------------ Neural network --------------------------------
set.seed(150) #For replication
mlp.fit = train(form = y~., #Use formula method to account for categorical variables
data = fdata_mlp.Reg.tr,
method = "nnet",
linout = TRUE,
# tuneGrid = data.frame(size =5, decay = 0),
tuneGrid = expand.grid(size = seq(5,15,length.out =3), decay =  10^(c(-3:0))),
maxit = 200,
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
mlp.fit #information about the resampling settings
ggplot(mlp.fit)+scale_x_log10() # ! - Pues aqui la verdad es que no me acuerdo muy bien.
plotnet(mlp.fit$finalModel) #Plot the network -> Aqui debemos de observar, en cual de ellas contribuye (Grosor de las lineas)
SensAnalysisMLP(mlp.fit) #Statistical sensitivity analysis -> Sensibilidad de contribución de las entradas.
set.seed(150) #For replication
set.seed(150) #For replication
mlp.fit2 = train(form = y~y_lag1+x_lag1+x_lag3, #Use formula method to account for categorical variables the most significant
data = fdata_mlp.Reg.tr,
method = "nnet",
linout = TRUE,
# tuneGrid = data.frame(size =5, decay = 0),
tuneGrid = expand.grid(size = seq(5,20,by=5), decay =  10^(seq(-3,0))),
maxit = 200,
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
mlp.fit2 #information about the resampling settings
ggplot(mlp.fit2)+scale_x_log10() # REDUCE NUMBER OF NEURONS?
plotnet(mlp.fit2$finalModel) #Plot the network -> Aqui debemos de observar, en cual de ellas contribuye (Grosor de las lineas)
SensAnalysisMLP(mlp.fit2) #Statistical sensitivity analysis -> Sensibilidad de contribución de las entradas.
# * 5. Predict Training Data + Observar discrepancias
#Predict training data
mlp_pred = predict(mlp.fit2,  newdata = fdata_mlp.Reg.tr)
# ? - Esto se encarga de observar la relación entre los factores y la predicción de la y
PlotModelDiagnosis(fdata_mlp.Reg.tr[,-1], fdata_mlp.Reg.tr[,1], mlp_pred, together = TRUE)
#Error measurements
accuracy(fdata.Reg.tr[,1],mlp_pred)
#Error measurements
accuracy(fdata_mlp.Reg.tr[,1],mlp_pred)
head(fdata_mlp.Reg.tr)
#Error measurements
accuracy(fdata_mlp.Reg.tr[,2],mlp_pred)
# ? - Esto se encarga de observar la relación entre los factores y la predicción de la y
PlotModelDiagnosis(fdata_mlp.Reg.tr[,-1], fdata_mlp.Reg.tr[,2], mlp_pred, together = TRUE)
#Error measurements
accuracy(fdata_mlp.Reg.tr[,2],mlp_pred)
head(fdata_mlp.Reg.tr)
plot(fdata.Reg.tr[21],type="l")
plot(fdata.Reg.tr[,2],type="l")
plot(fdata_mlp.Reg.tr[,2],type="l")
lines(mlp_pred,col = "red")
setwd("~/Desktop/My stuff/ICAI/ML 2/Retake_2021")
library(MLTools)
library(caret)
library(kernlab)
library(nnet)
library(NeuralNetTools)
library(NeuralSens)
library(Hmisc)
library(forecast)
z <- read.table("Prob2.dat", header=TRUE)
head(z)
ggtsdisplay(ts(z$y))
z <- read.table("Prob2.dat", header=TRUE)
head(z)
pros.dat =
read.table("http://www.stat.cmu.edu/~ryantibs/statcomp/data/pros.dat")
setwd("~/Desktop/My stuff/ICAI/Segundo Cuatri/Estadistica Comp/Intro-Estadistica-Computacional")
pros.dat =
read.table("http://www.stat.cmu.edu/~ryantibs/statcomp/data/pros.dat")
means <- sapply(pros.dat, mean, na.rm = TRUE) # Calculate the mean of each variable
sds <- sapply(pros.dat, sd, na.rm = TRUE) # Calculate the standard deviation of each variable
print(means)
print(sds)
mapply(function(x, y) plot(pros.dat$SVI, pros.dat[[x]], main=y, xlab="SVI", ylab=y), names(pros.dat)[-match("SVI", names(pros.dat))], names(pros.dat)[-match("SVI", names(pros.dat))])
lapply(names(pros.dat)[-which(names(pros.dat) == 'SVI')], function(x) plot(pros.dat[['SVI']], pros.dat[[x]], main = x, xlab = 'SVI', ylab=x))
lapply(names(pros.dat)[-which(names(pros.dat) == 'SVI')], function(x) plot(pros.dat[['SVI']], pros.dat[[x]], main = x, xlab = 'SVI', ylab=x))
lapply(names(pros.dat)[-which(names(pros.dat) == "svi")], function(x) plot(pros.dat[["svi"]], pros.dat[[x]], main = x, xlab = 'SVI', ylab=x))
lapply(names(pros.dat)[-which(names(pros.dat) == "svi")], function(x) plot(pros.dat[["svi"]], pros.dat[[x]], main = x, xlab = 'svi', ylab=x))
lapply(names(pros.dat)[-which(names(pros.dat) == "svi")], function(x) plot(pros.dat[["svi"]], pros.dat[[x]], main = x, xlab = 'svi', ylab=x))
mapply(function(x, y) plot(pros.dat$svi, pros.dat[[x]], main=y, xlab="svi", ylab=y), names(pros.dat)[-match("svi", names(pros.dat))], names(pros.dat)[-match("svi", names(pros.dat))])
#lapply(names(pros.dat)[-which(names(pros.dat) == "svi")], function(x) plot(pros.dat[["svi"]], pros.dat[[x]], main = x, xlab = 'svi', ylab=x))
mapply(function(x, y) plot(pros.dat$svi, pros.dat[[x]], main=y, xlab="svi", ylab=y), names(pros.dat)[-match("svi", names(pros.dat))], names(pros.dat)[-match("svi", names(pros.dat))])
t.test.by.ind = function(x, ind) {
stopifnot(all(ind %in% c(0, 1)))
return(t.test(x[ind == 0], x[ind == 1]))
}
t.test.by.ind = function(x, ind) {
stopifnot(all(ind %in% c(0, 1)))
return(t.test(x[ind == 0], x[ind == 1]))
}
# YOUR CODE GOES HERE
```
ind <- ifelse(pros.dat$svi > median(pros.dat$svi), 1, 0)
tests <- lapply(pros.dat[-which(names(pros.dat) == 'SVI')], function(x) t.test.by.ind(x,ind))
ind <- ifelse(pros.dat$svi > median(pros.dat$svi), 1, 0)
tests <- lapply(pros.dat[-which(names(pros.dat) == 'svi')], function(x) t.test.by.ind(x,ind))
print(tests)
p_values <- lapply(tests, function(x) x$p.value)
print(p_values)
rio = read.csv("http://www.stat.cmu.edu/~ryantibs/statcomp/data/rio.csv")
class(rio)
dim(rio)
names(rio)
class(rio) # data.fram
dim(rio)   # 11538 12
names(rio) #"id" "name" "nationality" "sex" "date_of_birth" "height" "weight" ...
anyNA(rio)
class(rio) # data.fram
dim(rio)   # 11538 12
names(rio) #"id" "name" "nationality" "sex" "date_of_birth" "height" "weight" ...
anyNA(rio) #TRUE
colSums(is.na(rio))
nrow(rio)
length(unique(rio&athlete_id))
nrow(rio)
length(unique(rio$id))
nrow(rio)
length(unique(rio$id))
length(unique(rio$Country))
nrow(rio)
length(unique(rio$id))
length(unique(rio$nationality))
nrow(rio)
length(unique(rio$nationality))
table(rio$Country)
nrow(rio)
length(unique(rio$nationality))
table(unique(rio$nationality))
country_counts <- table(rio$Country)
max_country <- names(which.max(country_counts))
max_count <- max(country_counts)
paste(max_country, max_count)
country_counts <- table(rio$nationality)
max_country <- names(which.max(country_counts))
max_count <- max(country_counts)
paste(max_country, max_count)
total_gold <- sum(rio$gold, na.rm = TRUE)
total_silver <- sum(rio$silver, na.rm = TRUE)
total_bronze <- sum(rio$bronze, na.rm = TRUE)
medal_counts <- c(Gold = total_gold, Silver = total_silver, Bronze = total_bronze)
medal_counts
total_gold <- sum(rio$gold, na.rm = TRUE)
total_silver <- sum(rio$silver, na.rm = TRUE)
total_bronze <- sum(rio$bronze, na.rm = TRUE)
medal_counts <- c(Gold = total_gold, Silver = total_silver, Bronze = total_bronze)
medal_counts
all.equal(medal_counts["Gold"], medal_counts["Silver"], medal_counts["Bronze"])
total_gold <- sum(rio$gold, na.rm = TRUE)
total_silver <- sum(rio$silver, na.rm = TRUE)
total_bronze <- sum(rio$bronze, na.rm = TRUE)
medal_counts <- c(Gold = total_gold, Silver = total_silver, Bronze = total_bronze)
medal_counts
all.equal(medal_counts["gold"], medal_counts["silver"], medal_counts["bronze"])
total_gold <- sum(rio$gold, na.rm = TRUE)
total_silver <- sum(rio$silver, na.rm = TRUE)
total_bronze <- sum(rio$bronze, na.rm = TRUE)
medal_counts <- c(Gold = total_gold, Silver = total_silver, Bronze = total_bronze)
medal_counts
all.equal(medal_counts["gold"], medal_counts["silver"], medal_counts["bronze"])
#The results are surprising since the number of gold, silver and bronze medals are different, however the function says that the number is equal. Why is this? Because all.equal is comparing the first element
is_equal <- (total_gold == total_silver) && (total_silver == total_bronze)
is_equal
# YOUR CODE GOES HERE
rio$total <- rio$gold + rio$silver + rio$bronze
max_total_medals <- max(rio$total)
athletes_most_total_medals <- rio$name[rio$total == max_total_medals]
max_gold_medals <- max(rio$gold)
athletes_most_gold_medals <- rio$name[rio$gold == max_gold_medals]
max_silver_medals <- max(rio$silver)
athletes_most_silver_medals <- rio$name[rio$silver == max_silver_medals]
cat("Athletes with the most total medals:", paste(athletes_most_total_medals, collapse = ", "), "\n")
cat("Number of total medals:", max_total_medals, "\n")
cat("Athletes with the most gold medals:", paste(athletes_most_gold_medals, collapse = ", "), "\n")
cat("Number of gold medals:", max_gold_medals, "\n")
cat("Athletes with the most silver medals:", paste(athletes_most_silver_medals, collapse = ", "), "\n")
cat("Number of silver medals:", max_silver_medals, "\n")
rio$total_medals <- rio$gold + rio$silver + rio$bronze
total.by.nat <- tapply(rio$total_medals, rio$nationality, sum)
print(total.by.nat)
max_medals <- max(total.by.nat)
country_most_medals <- names(total.by.nat)[total.by.nat == max_medals]
countries_zero_medals <- sum(total.by.nat == 0)
cat("Country with the most medals:", country_most_medals, "with", max_medals, "medals\n")
cat("Number of countries with zero medals:", countries_zero_medals, "\n")
countries_with_zero_medals <- names(total.by.nat[total.by.nat == 0])
athletes_count_zero_medals <- table(rio$nationality[rio$nationality %in% countries_with_zero_medals])
max_athletes_zero_medals <- max(athletes_count_zero_medals)
country_most_athletes_zero_medals <- names(athletes_count_zero_medals)[athletes_count_zero_medals == max_athletes_zero_medals]
cat("Country with the most athletes but zero medals:", country_most_athletes_zero_medals, "\n")
cat("Number of athletes:", max_athletes_zero_medals, "\n")
