TF.fit <- arima(y.TR,
order = c(2, 1, 0),
seasonal = list(order=c(0,0,2),period=12), #! - Change the period
xtransf = x.TR,
transfer = list(c(0,9)), # * List with (r,s) orders - If there are multiple x inputs, add multiple (r,s)
include.mean = TRUE,
method = "ML") #Maximum likelihood as a fitting method.
barplot(coef(TF.fit)[-(1:4)], las=2, col='darkblue') # The [-(1:2)] removes the ar1 and the incercept and just leaves us with the T1-MA
summary(TF.fit) # summary of training errors and estimated coefficients
coeftest(TF.fit)
# * 4.Check regression error to see the need of differentiation
TF.RegressionError.plot(y.TR,x.TR,TF.fit,lag.max = 50)
# * 5. Check numerator coefficients of explanatory variable
TF.Identification.plot(x,TF.fit)
TF.fit <- arima(y.TR,
order = c(2, 1, 0),
seasonal = list(order=c(0,0,1),period=12), #! - Change the period
xtransf = x.TR,
transfer = list(c(1,1)), # * List with (r,s) orders - If there are multiple x inputs, add multiple (r,s)
include.mean = TRUE,
method = "ML") #Maximum likelihood as a fitting method.
barplot(coef(TF.fit), las=2, col='darkblue') # The [-(1:2)] removes the ar1 and the incercept and just leaves us with the T1-MA
summary(TF.fit) # summary of training errors and estimated coefficients
coeftest(TF.fit)
# * 7. Check residuals
CheckResiduals.ICAI(TF.fit, lag=50) #! Nice, white noise achieved :-)
ggtsdisplay(TF.fit$residuals)
#* 8. Check Cross correlation residuals - expl. variables
res <- residuals(TF.fit)
res[is.na(res)] <- 0
ccf(y = res, x = x.TR)
# Diagnosis del modelo generado:
autoplot(y.TR, series='Data') +
autolayer(fitted(TF.fit), series = "FIT")
#Obtain forecast for horizon = 7 using the trained parameters of the model
val.forecast_h12 <- TF.forecast(y.old = y.TR, #past values of the series
x.old = x.TR, #Past values of the explanatory variables
x.new = x.TV, #New values of the explanatory variables
model = TF.fit, #fitted transfer function model
h=12) #Forecast horizon
tail(y.TR*10)
val.forecast_h12*10
# Sigue una continuacion logica para el forecast:
autoplot(cbind(y.TV, x.TV), facets=TRUE)
y.TV.est <- rep(NA, length(y.TV))
for (i in seq(length(y.TR)+1, length(y), 1)){
y.TV.est[i] <- TF.forecast(y.old = subset(y, end=i-1),
x.old = subset(x, end = i-1),
x.new = subset(x, start = i, end = i),
model = TF.fit,
h = 1)
}
y.TV.est <- na.omit(y.TV.est)
head(y.TV.est)
head(y.TV)
accuracy(y.TV.est, y.TV*10)
# RMSE - 10.11478
plot(y.TV*10)
plot(y.TV.est)
#Plot series and forecast (! Important)
autoplot(y.TV*10)+
forecast::autolayer(ts(y.TV.est))
# Bueno, hora de pasar al siguiente ejercicio que se me hace tarde:
#Problem 2: (5 points)
#The dataset Prob2.dat contains two non-seasonal time series: the input x and the output y.
#Questions:
#  1. Identify a nonlinear model to predict one step ahead values of the output variable y,
#     using 80% of the data.
#  2. Diagnose your proposed model.
#  3. Validate your model using the remaining 20% of the data.
fdata_mlp <- read.table("Prob2.dat", header = TRUE) # Si el archivo .dat chueco.
head(fdata_mlp)
fdata_ts_mlp <- ts(fdata_mlp)
y.mlp <- fdata_ts_mlp[,2]
x.mlp <- fdata_ts_mlp[,1]
y.mlp.TR <- subset(y.mlp, end = length(y.mlp)*0.8)
y.mlp.TV <- subset(y.mlp, start = length(y.mlp)*0.8 + 1 )
x.mlp.TR <- subset(x.mlp, end = length(x.mlp)*0.8)
x.mlp.TV <- subset(x.mlp, start = length(x.mlp)*0.8 + 1)
"contains two non-seasonal time series"
# Genial, vamos a ver ahora. Dudas dudas.
ggtsdisplay(y)
# Vale, aqui va a hacer falta diferenciacion como la copa de un pino. <- Que expresion tan curiosa.
ggtsdisplay(x)
fdata.REG <- fdata_ts[]
#"contains two non-seasonal time series"
# Genial, vamos a ver ahora. Dudas dudas.
ggtsdisplay(y.mlp.TR)
# Vale, aqui va a hacer falta diferenciacion como la copa de un pino. <- Que expresion tan curiosa.
ggtsdisplay(x.mlp.TR)
fdata_mlp.REG <- fdata_ts_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg <- fdata_ts_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
fdata_mlp.Reg <- fdata_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x.mlp.TR,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x.mlp.TR,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x.mlp.TR,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y.mlp.TR,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y.mlp.TR,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y.mlp.TR,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
fdata.Reg.tr <- fdata.Reg
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x.mlp.TR,1)
# Bueno, hora de pasar al siguiente ejercicio que se me hace tarde:
#Problem 2: (5 points)
#The dataset Prob2.dat contains two non-seasonal time series: the input x and the output y.
#Questions:
#  1. Identify a nonlinear model to predict one step ahead values of the output variable y,
#     using 80% of the data.
#  2. Diagnose your proposed model.
#  3. Validate your model using the remaining 20% of the data.
fdata_mlp <- read.table("Prob2.dat", header = TRUE) # Si el archivo .dat chueco.
head(fdata_mlp)
fdata_ts_mlp <- ts(fdata_mlp)
y.mlp <- fdata_ts_mlp[,2]
x.mlp <- fdata_ts_mlp[,1]
y.mlp.TR <- subset(y.mlp, end = length(y.mlp)*0.8)
y.mlp.TV <- subset(y.mlp, start = length(y.mlp)*0.8 + 1 )
x.mlp.TR <- subset(x.mlp, end = length(x.mlp)*0.8)
x.mlp.TV <- subset(x.mlp, start = length(x.mlp)*0.8 + 1)
#"contains two non-seasonal time series"
# Genial, vamos a ver ahora. Dudas dudas.
ggtsdisplay(y.mlp.TR)
# Vale, aqui va a hacer falta diferenciacion como la copa de un pino. <- Que expresion tan curiosa.
ggtsdisplay(x.mlp.TR)
# Esto que no sea la time series
fdata_mlp.Reg <- fdata_mlp[,c(1,2)]
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y, lag.max = 100)
#Vamos a diferenciar las series a ver que nos sale
#* 9: En caso de que no lleve fecha - Diferenciar para averiguar la ventana temporal. z for Box-Cox, y for normal.
# Differentiation: if the ACF decreases very slowly -> needs differenciation
ggtsdisplay(y.mlp, lag.max = 100)
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(y.mlp, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12) # differences contains the order of differentiation
# ? - Parte I: Diferenciación Regular  - d
Bz <- diff(x.mlp, differences = 1)
ggtsdisplay(Bz, lag.max = 4 * 12)
# Let's go!
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x.mlp.TR,1)
# Let's go! (Vale, entonces, e)
fdata_mlp.Reg$x_lag1 <- Lag(x.mlp.TR,1)
fdata_mlp.Reg$x_lag2 <- Lag(x.mlp.TR,2)
fdata_mlp.Reg$x_lag3 <- Lag(x.mlp.TR,3)
# Let's go! (Vale, entonces esto se tiene que hacer con la columna entera, no la time series de entrenamiento)
fdata_mlp.Reg$x_lag1 <- Lag(fdata_mlp$x,1)
fdata_mlp.Reg$x_lag2 <- Lag(fdata_mlp$x,2)
fdata_mlp.Reg$x_lag3 <- Lag(fdata_mlp$x,3)
fdata_mlp.Reg$y_lag1 <- Lag(fdata_mlp$y,1)
fdata_mlp.Reg$y_lag2 <- Lag(fdata_mlp$y,2)
fdata_mlp.Reg$y_lag3 <- Lag(fdata_mlp$y,3)
# ? - Part II Remove the NA's caused by the lag
#Notice that the begining of the time series contains NA due to the new lagged series
head(fdata_mlp.Reg)
fdata.Reg.tr <- fdata.Reg
#Remove missing values
fdata.Reg.tr <- na.omit(fdata.Reg.tr)
fdata_mlp.Reg.tr <- fdata_mlp.Reg
#Remove missing values
fdata_mlp.Reg.tr <- na.omit(fdata_mlp.Reg.tr)
fdata_mlp.Reg.tr
# * 3. Set up the Cross-Validation with 10 folds
ctrl_tune <- trainControl(method = "cv",
number = 10,
summaryFunction = defaultSummary,    #Performance summary for comparing models in hold-out samples.
savePredictions = TRUE)              #save predictions
# * 4. Neural Network setup [TRAINING]
#------------ Neural network --------------------------------
set.seed(150) #For replication
mlp.fit = train(form = y~., #Use formula method to account for categorical variables
data = fdata_mlp.Reg.tr,
method = "nnet",
linout = TRUE,
# tuneGrid = data.frame(size =5, decay = 0),
tuneGrid = expand.grid(size = seq(5,15,length.out =3), decay =  10^(c(-3:0))),
maxit = 200,
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
mlp.fit #information about the resampling settings
ggplot(mlp.fit)+scale_x_log10() # ! - Pues aqui la verdad es que no me acuerdo muy bien.
plotnet(mlp.fit$finalModel) #Plot the network -> Aqui debemos de observar, en cual de ellas contribuye (Grosor de las lineas)
SensAnalysisMLP(mlp.fit) #Statistical sensitivity analysis -> Sensibilidad de contribución de las entradas.
set.seed(150) #For replication
set.seed(150) #For replication
mlp.fit2 = train(form = y~y_lag1+x_lag1+x_lag3, #Use formula method to account for categorical variables the most significant
data = fdata_mlp.Reg.tr,
method = "nnet",
linout = TRUE,
# tuneGrid = data.frame(size =5, decay = 0),
tuneGrid = expand.grid(size = seq(5,20,by=5), decay =  10^(seq(-3,0))),
maxit = 200,
preProcess = c("center","scale"),
trControl = ctrl_tune,
metric = "RMSE")
mlp.fit2 #information about the resampling settings
ggplot(mlp.fit2)+scale_x_log10() # REDUCE NUMBER OF NEURONS?
plotnet(mlp.fit2$finalModel) #Plot the network -> Aqui debemos de observar, en cual de ellas contribuye (Grosor de las lineas)
SensAnalysisMLP(mlp.fit2) #Statistical sensitivity analysis -> Sensibilidad de contribución de las entradas.
# * 5. Predict Training Data + Observar discrepancias
#Predict training data
mlp_pred = predict(mlp.fit2,  newdata = fdata_mlp.Reg.tr)
# ? - Esto se encarga de observar la relación entre los factores y la predicción de la y
PlotModelDiagnosis(fdata_mlp.Reg.tr[,-1], fdata_mlp.Reg.tr[,1], mlp_pred, together = TRUE)
#Error measurements
accuracy(fdata.Reg.tr[,1],mlp_pred)
#Error measurements
accuracy(fdata_mlp.Reg.tr[,1],mlp_pred)
head(fdata_mlp.Reg.tr)
#Error measurements
accuracy(fdata_mlp.Reg.tr[,2],mlp_pred)
# ? - Esto se encarga de observar la relación entre los factores y la predicción de la y
PlotModelDiagnosis(fdata_mlp.Reg.tr[,-1], fdata_mlp.Reg.tr[,2], mlp_pred, together = TRUE)
#Error measurements
accuracy(fdata_mlp.Reg.tr[,2],mlp_pred)
head(fdata_mlp.Reg.tr)
plot(fdata.Reg.tr[21],type="l")
plot(fdata.Reg.tr[,2],type="l")
plot(fdata_mlp.Reg.tr[,2],type="l")
lines(mlp_pred,col = "red")
setwd("~/Desktop/My stuff/ICAI/ML 2/Retake_2021")
library(MLTools)
library(caret)
library(kernlab)
library(nnet)
library(NeuralNetTools)
library(NeuralSens)
library(Hmisc)
library(forecast)
z <- read.table("Prob2.dat", header=TRUE)
head(z)
ggtsdisplay(ts(z$y))
z <- read.table("Prob2.dat", header=TRUE)
head(z)
# A hook to wrap output based on a linewidth chunk option
# From https://github.com/yihui/knitr-examples/blob/master/077-wrap-output.Rmd
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
# this hook is used only when the linewidth option is not NULL
if (!is.null(n <- options$linewidth)) {
x = knitr:::split_lines(x)
# any lines wider than n should be wrapped
if (any(nchar(x) > n)) x = strwrap(x, width = n)
x = paste(x, collapse = '\n')
}
hook_output(x, options)
})
opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE, linewidth=79)
setwd("~/Desktop/My stuff/ICAI/Segundo Cuatri/Estadistica Comp/Intro-Estadistica-Computacional/Lab2_Parallel_Computing")
# A hook to wrap output based on a linewidth chunk option
# From https://github.com/yihui/knitr-examples/blob/master/077-wrap-output.Rmd
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
# this hook is used only when the linewidth option is not NULL
if (!is.null(n <- options$linewidth)) {
x = knitr:::split_lines(x)
# any lines wider than n should be wrapped
if (any(nchar(x) > n)) x = strwrap(x, width = n)
x = paste(x, collapse = '\n')
}
hook_output(x, options)
})
opts_chunk$set(cache=TRUE, autodep=TRUE, cache.comments=TRUE, linewidth=79)
setwd("~/Desktop/My stuff/ICAI/Segundo Cuatri/Estadistica Comp/Intro-Estadistica-Computacional/Lab2_Parallel_Computing")
# YOUR CODE GOES HERE
library(doParallel)
# YOUR CODE GOES HERE
# Load the parallel package
library(parallel)
# Determine the number of cores
num_cores <- detectCores()
# Create no_cores variable
no_cores <- num_cores - 1
# Print the result
print(no_cores)
# Load necessary libraries
library(parallel)
library(doParallel)
# Determine the number of cores (assuming you've already calculated no_cores)
num_cores <- detectCores()
no_cores <- num_cores - 1
# Create a cluster using makeCluster() based on the number of cores we want to use
cl <- makeCluster(no_cores)
# Register the cluster with registerDoParallel() so foreach knows to use it
registerDoParallel(cl)
library(parallel)
library(doParallel)
library(foreach)
library(iterators) # For iteration in foreach
library(microbenchmark)
# Define the function to get prime numbers
getPrimeNumbers <- function(n) {
n <- as.integer(n)
if(n > 1e6) stop("n too large")
primes <- rep(TRUE, n)
primes[1] <- FALSE
last.prime <- 2L
for(i in last.prime:floor(sqrt(n))) {
primes[seq.int(2L*last.prime, n, last.prime)] <- FALSE
last.prime <- last.prime + min(which(primes[(last.prime+1):n]))
}
which(primes)
}
# lapply and for loop definitions as provided
lapplyPrimeNumbers <- function(n_vec) {
result <- lapply(n_vec, getPrimeNumbers)
}
forPrimeNumbers <- function(n_vec) {
result <- list()
for (n in n_vec) {
result[[as.character(n)]] <- getPrimeNumbers(n)
}
}
# Setting up parallel backend
num_cores <- detectCores()
no_cores <- num_cores - 1
cl <- makeCluster(no_cores)
registerDoParallel(cl)
# parLapply implementation
parLapplyPrimeNumbers <- function(n_vec) {
result <- parLapply(cl, n_vec, getPrimeNumbers)
}
# foreach implementation
foreachPrimeNumbers <- function(n_vec) {
result <- foreach(n = iter(n_vec), .combine = 'c') %dopar% getPrimeNumbers(n)
}
# Benchmarking
results <- microbenchmark(
lapply = lapplyPrimeNumbers(10:1000),
forLoop = forPrimeNumbers(10:1000),
parLapply = parLapplyPrimeNumbers(10:1000),
foreach = foreachPrimeNumbers(10:1000),
times = 10 # Adjust the number of times to ensure the benchmark is comprehensive
)
library(parallel)
library(doParallel)
library(foreach)
library(iterators) # For iteration in foreach
library(microbenchmark)
# Define the function to get prime numbers
getPrimeNumbers <- function(n) {
n <- as.integer(n)
if(n > 1e6) stop("n too large")
primes <- rep(TRUE, n)
primes[1] <- FALSE
last.prime <- 2L
for(i in last.prime:floor(sqrt(n))) {
primes[seq.int(2L*last.prime, n, last.prime)] <- FALSE
last.prime <- last.prime + min(which(primes[(last.prime+1):n]))
}
which(primes)
}
# lapply and for loop definitions as provided
lapplyPrimeNumbers <- function(n_vec) {
result <- lapply(n_vec, getPrimeNumbers)
}
forPrimeNumbers <- function(n_vec) {
result <- list()
for (n in n_vec) {
result[[as.character(n)]] <- getPrimeNumbers(n)
}
}
# Setting up parallel backend
num_cores <- detectCores()
no_cores <- num_cores - 1
cl <- makeCluster(no_cores)
clusterExport(cl, varlist = c("getPrimeNumbers"))
registerDoParallel(cl)
# parLapply implementation
parLapplyPrimeNumbers <- function(n_vec) {
result <- parLapply(cl, n_vec, getPrimeNumbers)
}
# foreach implementation
foreachPrimeNumbers <- function(n_vec) {
result <- foreach(n = iter(n_vec), .combine = 'c') %dopar% getPrimeNumbers(n)
}
# Benchmarking
results <- microbenchmark(
lapply = lapplyPrimeNumbers(10:1000),
forLoop = forPrimeNumbers(10:1000),
parLapply = parLapplyPrimeNumbers(10:1000),
foreach = foreachPrimeNumbers(10:1000),
times = 10 # Adjust the number of times to ensure the benchmark is comprehensive
)
print(results)
# Clean up: stop the cluster
stopCluster(cl)
library(doParallel)
library(foreach)
library(glmnet) # Assuming glmnet is needed for glm, adjust as necessary for your environment
# Load necessary library for timing
library(microbenchmark)
# Detect the number of cores
no_cores <- detectCores() - 1
# Register the parallel backend
registerDoParallel(no_cores)
# Prepare the data
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- seq(1, 10000)
# Parallel version of the boot_fx function
boot_fx_parallel <- function(trials) {
results <- foreach(trial=trials, .combine=rbind) %dopar% {
ind <- sample(100, 100, replace=TRUE)
result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
r <- coefficients(result1)
res <- rbind(data.frame(), r)
}
return(results)
}
# Benchmark the original function
sequential_time <- microbenchmark(boot_fx(trials), times = 1)
library(doParallel)
library(foreach)
library(glmnet) # Assuming glmnet is needed for glm, adjust as necessary for your environment
# Load necessary library for timing
library(microbenchmark)
# Detect the number of cores
no_cores <- detectCores() - 1
# Register the parallel backend
registerDoParallel(no_cores)
# Prepare the data
x <- iris[which(iris[,5] != "setosa"), c(1,5)]
trials <- seq(1, 10000)
boot_fx <- function(trial) {
ind <- sample(100, 100, replace=TRUE)
result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
r <- coefficients(result1)
res <- rbind(data.frame(), r)
}
# Parallel version of the boot_fx function
boot_fx_parallel <- function(trials) {
results <- foreach(trial=trials, .combine=rbind) %dopar% {
ind <- sample(100, 100, replace=TRUE)
result1 <- glm(x[ind,2]~x[ind,1], family=binomial(logit))
r <- coefficients(result1)
res <- rbind(data.frame(), r)
}
return(results)
}
# Benchmark the original function
sequential_time <- microbenchmark(boot_fx(trials), times = 1)
# Benchmark the parallel function
parallel_time <- microbenchmark(boot_fx_parallel(trials), times = 1)
# Print results
print(sequential_time)
print(parallel_time)
# Calculate speedup
speedup <- sequential_time$time / parallel_time$time
print(paste("Speedup:", speedup))
